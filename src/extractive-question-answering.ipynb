{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "casual-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is a way of passing a passage to the computer containing various answers and statements, and then asking \\nthe questions based on the facts in the passage gives, understand it and telling the answers by extracting it and\\norganizing it in an meaningful way.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a way of passing a passage to the computer containing various answers and statements, and then asking \n",
    "the questions based on the facts in the passage gives, understand it and telling the answers by extracting it and\n",
    "organizing it in an meaningful way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "proof-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "constitutional-cradle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d122a655f33842b5a8b2a1ffbe4f2f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66525bad0764dc5816c2a885093a291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac8d2dd5e604f21ad75093e93ceaa52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a06f506c84f43d3b58fc51b7c9d411e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")  # Use the question answering module from the library for answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "center-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the context or the passage, from which the computer is supposed to answer the questions given from.\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "varied-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(question, context=context):\n",
    "    result = nlp(question=question, context=context)\n",
    "    \n",
    "    answer = result['answer']\n",
    "    score = round(result['score'], 4)  # Round the score to 3 decimals\n",
    "    start = result['start']\n",
    "    end = result['end']\n",
    "    \n",
    "    return answer, score, start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "superb-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is extractive question answering?\",\n",
    "    \"What is a good example of a question answering dataset?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "arabic-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    answer, score, start, end = extract_answer(question)\n",
    "    \n",
    "    print(f\"Answer: {answer} with score of {score} [start: {}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
